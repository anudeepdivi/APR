"""APRecog.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18HghY2zQlI8hgmk3wKel1nia8m5LrcEu
"""

import zipfile
with zipfile.ZipFile('/content/Licplatesrecognition_train.zip', 'r') as zip_ref:
  zip_ref.extractall()



import os
import pandas as pd
from PIL import Image
import torch
from torch.utils.data import Dataset
from transformers import (
    TrOCRProcessor,
    VisionEncoderDecoderModel,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer
)

# 1. Create a PyTorch Dataset
class LicensePlateOCRDataset(Dataset):
    def __init__(self, csv_file, img_dir, processor, max_target_length=16):
        self.df = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.processor = processor
        self.max_target_length = max_target_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        filename = row['img_id']  
        label = str(row['text'])
        img_path = os.path.join(self.img_dir, filename)
        image = Image.open(img_path).convert("RGB")
        # Process image to obtain pixel values
        pixel_values = self.processor(images=image, return_tensors="pt").pixel_values.squeeze()
        encoding = self.processor.tokenizer(
            label,
            padding="max_length",
            truncation=True,
            max_length=self.max_target_length,
            return_tensors="pt"
        )
        labels = encoding.input_ids.squeeze()
        labels = torch.where(labels == self.processor.tokenizer.pad_token_id, -100, labels)
        return {"pixel_values": pixel_values, "labels": labels}

# ------------------------------
# 2. Load Pretrained Model & Processor
# ------------------------------
csv_path = "/content/Licplatesrecognition_train.csv"
img_dir = "/content/license_plates_recognition_train"

processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-printed")
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-printed")
model.config.decoder_start_token_id = processor.tokenizer.bos_token_id
model.config.pad_token_id = processor.tokenizer.pad_token_id


if hasattr(model.encoder.config, "gradient_checkpointing"):
    model.encoder.config.gradient_checkpointing = True
if hasattr(model.decoder.config, "gradient_checkpointing"):
    model.decoder.config.gradient_checkpointing = True

# ------------------------------
# 3. Prepare Dataset & Data Collator
# ------------------------------
dataset = LicensePlateOCRDataset(csv_path, img_dir, processor, max_target_length=16)


def data_collator(batch):
    pixel_values = torch.stack([item["pixel_values"] for item in batch])
    labels = torch.stack([item["labels"] for item in batch])
    return {"pixel_values": pixel_values, "labels": labels}

# ------------------------------
# 4. Define Training Arguments & Trainer
# ------------------------------
training_args = Seq2SeqTrainingArguments(
    output_dir="./trocr-finetuned",
    per_device_train_batch_size=2,           # reduced batch size
    gradient_accumulation_steps=4,             # effective batch size = 2*4 = 8
    predict_with_generate=True,
    num_train_epochs=3,
    logging_steps=50,
    save_steps=500,
    evaluation_strategy="no",
    fp16=True,                                
    dataloader_num_workers=0,
    report_to="tensorboard",
    learning_rate=5e-5
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator,
    tokenizer=processor.tokenizer,
)

# ------------------------------
# 5. Train & Save the Model
# ------------------------------
def main():
    trainer.train()
    save_dir = "./trocr-finetuned"
    model.save_pretrained(save_dir)
    processor.save_pretrained(save_dir)
    print("Training complete. Model and processor saved to", save_dir)

if __name__ == "__main__":
    main()

