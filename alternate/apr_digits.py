# -*- coding: utf-8 -*-
"""APR_digits.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Kl78683jeUKhYIdH_GEzDPbizbBgrXK
"""

# -*- coding: utf-8 -*-
"""APR_digits.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Kl78683jeUKhYIdH_GEzDPbizbBgrXK
"""

import os
import cv2
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import zipfile
import random
import matplotlib.pyplot as plt



#####################################
# Dataset and DataLoader Definition #
#####################################

class LicensePlateDataset(Dataset):
    def __init__(self, csv_file, img_dir, target_height=32):
        """
        Args:
            csv_file (str): Path to CSV file with columns 'filename' and 'label'
            img_dir (str): Directory with images.
            target_height (int): Height to which images are resized.
        """
        self.df = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.target_height = target_height
        # Define alphabet (digits 0-9 and letter 'T')
        self.alphabet = "0123456789T"
        # Create mapping: character -> integer (1-indexed; 0 is reserved for blank)
        self.char_to_num = {char: i+1 for i, char in enumerate(self.alphabet)}

    def preprocess_image(self, image):
        """Resize grayscale image to fixed height while preserving aspect ratio."""
        h, w = image.shape
        scale = self.target_height / float(h)
        new_width = int(w * scale)
        resized = cv2.resize(image, (new_width, self.target_height))
        return resized

    def encode_label(self, text):
        """Convert text label into a list of integers."""
        return [self.char_to_num[char] for char in text if char in self.char_to_num]

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        filename = row['img_id']
        label_str = str(row['text'])
        img_path = os.path.join(self.img_dir, filename)
        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if image is None:
            raise ValueError(f"Image not found: {img_path}")
        # Preprocess image
        image = self.preprocess_image(image)
        image = image.astype(np.float32) / 255.0  # Normalize to [0,1]
        # Add channel dimension -> shape: (1, H, W)
        image = np.expand_dims(image, axis=0)
        label_encoded = self.encode_label(label_str)
        sample = {
            'image': torch.tensor(image),            # (1, target_height, width)
            'label': torch.tensor(label_encoded, dtype=torch.long),
            'label_str': label_str
        }
        return sample

def collate_fn(batch):
    """
    Pads images (along width) and labels in a batch.
    Returns:
      images: (batch, 1, H, max_width)
      padded_labels: (batch, max_label_length)
      input_lengths: (batch,) - predicted time steps from CNN.
      target_lengths: (batch,) - actual label lengths.
      label_strs: list of ground truth strings.
    """
    images = [b['image'] for b in batch]
    labels = [b['label'] for b in batch]
    label_strs = [b['label_str'] for b in batch]
    batch_size = len(images)

    # Pad images along width dimension (all images: shape (1, H, W))
    max_width = max([img.shape[2] for img in images])
    padded_images = torch.zeros(batch_size, 1, images[0].shape[1], max_width)
    for i, img in enumerate(images):
        _, H, W = img.shape
        padded_images[i, :, :, :W] = img

    # Compute input_lengths for CTC.
    # Based on our CNN architecture below, the width is reduced by a factor of 4 and then reduced by 1.
    # So for padded image width = max_width, predicted time steps T = (max_width // 4) - 1.
    time_steps = (max_width // 4) - 1
    time_steps = time_steps if time_steps > 0 else 1
    input_lengths = torch.full((batch_size,), time_steps, dtype=torch.long)

    # Pad labels to maximum label length in this batch.
    max_label_len = max([len(lbl) for lbl in labels])
    padded_labels = torch.zeros(batch_size, max_label_len, dtype=torch.long)
    target_lengths = []
    for i, lbl in enumerate(labels):
        padded_labels[i, :len(lbl)] = lbl
        target_lengths.append(len(lbl))
    target_lengths = torch.tensor(target_lengths, dtype=torch.long)

    return padded_images, padded_labels, input_lengths, target_lengths, label_strs

####################
# Model Definition #
####################

class CRNN(nn.Module):
    def __init__(self, imgH, n_channels, n_classes, nh):
        """
        Args:
            imgH (int): Fixed image height (e.g. 32)
            n_channels (int): Number of input channels (1 for grayscale)
            n_classes (int): Number of output classes (alphabet size + 1 for blank)
            nh (int): Number of hidden units in LSTM.
        """
        super(CRNN, self).__init__()
        # CNN part
        self.conv1 = nn.Sequential(
            nn.Conv2d(n_channels, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)  # output: (64, H/2, W/2)
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)  # output: (128, H/4, W/2)
        )
        self.conv3 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU()
        )
        self.conv4 = nn.Sequential(
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)  # output: (256, H/8, W/4)
        )
        self.conv5 = nn.Sequential(
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU()
        )
        self.conv6 = nn.Sequential(
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)  # output: (512, H/16, W/4)
        )
        self.conv7 = nn.Sequential(
            nn.Conv2d(512, 512, kernel_size=2, stride=1, padding=0),
            nn.BatchNorm2d(512),
            nn.ReLU()
        )
        # After conv7, if input H=32, height should become 1.

        # LSTM: Input size is 512 (from CNN), hidden size nh, bidirectional.
        self.lstm = nn.LSTM(512, nh, bidirectional=True, num_layers=1)
        self.fc = nn.Linear(nh * 2, n_classes)  # mapping to n_classes

    def forward(self, x):
        # x: (batch, 1, H, W)
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.conv5(x)
        x = self.conv6(x)
        x = self.conv7(x)
        # x shape: (batch, 512, H', W'). Ideally H' == 1.
        x = x.squeeze(2)  # remove height dimension -> (batch, 512, W')
        x = x.permute(2, 0, 1)  # (W', batch, 512) --> W' is time steps
        # Pass through LSTM
        x, _ = self.lstm(x)  # output: (T, batch, 2*nh)
        T, batch, _ = x.size()
        x = x.view(T * batch, -1)
        x = self.fc(x)
        x = x.view(T, batch, -1)  # (T, batch, n_classes)
        return x

#######################
# Inference Functions #
#######################

def ocr_inference(model, image, device, target_height=32):
    """
    Given a grayscale image (numpy array), preprocess and run through the model,
    then decode predictions using greedy decoding.
    """
    model.eval()
    # Preprocess image (reuse the dataset's method)
    h, w = image.shape
    scale = target_height / float(h)
    new_width = int(w * scale)
    image_proc = cv2.resize(image, (new_width, target_height))
    image_proc = image_proc.astype(np.float32) / 255.0
    image_proc = np.expand_dims(image_proc, axis=0)  # (H, W)
    image_proc = np.expand_dims(image_proc, axis=0)  # (1, H, W)
    image_tensor = torch.tensor(image_proc, dtype=torch.float32).to(device)

    with torch.no_grad():
        y_pred = model(image_tensor)  # (T, 1, n_classes)
        y_pred = F.log_softmax(y_pred, dim=2)
        # Get argmax at each time step
        y_pred = y_pred.cpu()
        preds = y_pred.argmax(dim=2)  # (T, 1)
        preds = preds.squeeze(1)      # (T,)

    # Remove consecutive duplicates and blanks (0)
    pred_indices = []
    prev = None
    for idx in preds:
        idx = idx.item()
        if idx != prev and idx != 0:
            pred_indices.append(idx)
        prev = idx
    # Map indices to characters (inverse mapping)
    num_to_char = {i+1: char for i, char in enumerate("0123456789T")}
    result = "".join([num_to_char[i] for i in pred_indices if i in num_to_char])
    return result

def preprocess_image(image, target_height=32):
    """
    Resize a grayscale image to a fixed height while preserving the aspect ratio.
    """
    h, w = image.shape
    scale = target_height / float(h)
    new_width = int(w * scale)
    resized = cv2.resize(image, (new_width, target_height))
    return resized

def decode_predictions(pred_indices, alphabet="0123456789T"):
    """
    Decodes a sequence of prediction indices to text using greedy decoding.
    Assumes blank token is 0 and the mapping is 1-indexed.
    """
    decoded = []
    previous = None
    for idx in pred_indices:
        idx = idx.item()  # Convert from tensor to Python int
        if idx != previous and idx != 0:  # Skip duplicates and blank token
            if idx - 1 < len(alphabet):
                decoded.append(alphabet[idx - 1])
        previous = idx
    return "".join(decoded)

#########################################
# Main Block: Training, Saving, Inference
#########################################

if __name__ == "__main__":
    # Set up device and hyperparameters
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    imgH = 32
    n_channels = 1
    alphabet = "0123456789T"
    n_classes = len(alphabet) + 1  # extra blank token
    nh = 256
    batch_size = 8
    num_epochs = 10

    # Instantiate dataset and DataLoader
    csv_path = '/content/Licplatesrecognition_train.csv'
    img_dir = '/content/license_plates_recognition_train'
    # Extract the dataset zip file
    with zipfile.ZipFile('/content/Licplatesrecognition_train.zip', 'r') as zip_ref:
        zip_ref.extractall()
    dataset = LicensePlateDataset(csv_path, img_dir, target_height=imgH)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)

    # Instantiate model, optimizer, and CTC loss
    model = CRNN(imgH, n_channels, n_classes, nh).to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)

    # Initial training loop
    model.train()
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for batch in dataloader:
            images, labels, input_lengths, target_lengths, _ = batch
            images = images.to(device)
            labels = labels.to(device)
            input_lengths = input_lengths.to(device)
            target_lengths = target_lengths.to(device)

            optimizer.zero_grad()
            y_pred = model(images)  # output shape: (T, batch, n_classes)
            y_pred = F.log_softmax(y_pred, dim=2)
            loss = ctc_loss(y_pred, labels, input_lengths, target_lengths)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(dataloader)
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")

    # Example inference on a sample from the dataset
    sample_info = dataset[0]
    sample_filename = dataset.df.iloc[0]['img_id']
    sample_img_path = os.path.join(img_dir, sample_filename)
    sample_img = cv2.imread(sample_img_path, cv2.IMREAD_GRAYSCALE)
    if sample_img is not None:
        predicted_text = ocr_inference(model, sample_img, device, target_height=imgH)
        print("Ground Truth:", sample_info['label_str'])
        print("Predicted Text:", predicted_text)
    else:
        print("Sample image could not be loaded.")

    # (Optional) Continue training for additional epochs if needed
    start_epoch = 11  # Assuming you want to continue training (adjust as necessary)
    end_epoch = 23
    model.train()
    for epoch in range(start_epoch, end_epoch):
        epoch_loss = 0.0
        for batch in dataloader:
            images, labels, input_lengths, target_lengths, _ = batch
            images = images.to(device)
            labels = labels.to(device)
            input_lengths = input_lengths.to(device)
            target_lengths = target_lengths.to(device)

            optimizer.zero_grad()
            y_pred = model(images)
            y_pred = F.log_softmax(y_pred, dim=2)
            loss = ctc_loss(y_pred, labels, input_lengths, target_lengths)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(dataloader)
        print(f"Epoch {epoch+1}/{end_epoch}, Loss: {avg_loss:.4f}")

    # Save the model (state dict and entire model)

    save_path_complete = "/content/APR_OCR_complete.pth"
    torch.save(model, save_path_complete)

